{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASX dataset - 2018",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChristoferAdrian/Annual-report-readability/blob/main/ASX_dataset_2018.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAaqfVP1Ke0r"
      },
      "source": [
        "# Codes to extract 2018 ASX annual reports\n",
        "\n",
        "##### Remember to change the file names & paths from Step 2 to Step 5, according the year"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8OxPJ9j_Jal"
      },
      "source": [
        "## Step 1: Import all necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izEVi1wasicJ"
      },
      "source": [
        "!pip install textstat\n",
        "!pip install PyPDF2\n",
        "!pip install -U textblob\n",
        "!pip install pdfplumber\n",
        "import os\n",
        "import glob\n",
        "import textstat\n",
        "import csv\n",
        "import PyPDF2\n",
        "from textblob import TextBlob\n",
        "import pdfplumber   # read PDF files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeSEa0jh_tWK"
      },
      "source": [
        "## Step 2: Connect to my Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1mRC6_qsofR"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIq-YKSC_920"
      },
      "source": [
        "Change to the path of the selected folder here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua612vczs8BJ"
      },
      "source": [
        "TARGET_FILES = r'/content/drive/MyDrive/ASX annual reports/2018 (1772 firms)/*.pdf'\n",
        "file_list = glob.glob(TARGET_FILES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7muXRbOun9U7"
      },
      "source": [
        "print(file_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx3afZbVAN2b"
      },
      "source": [
        "Check the number of files in this folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZc7OJure1px"
      },
      "source": [
        "len(file_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oRlSJ9bAkDh"
      },
      "source": [
        "## Step 3: Extract PDF files (around 500 each time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oCRr5GlKDXp"
      },
      "source": [
        "### Step 3.1: List 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blqeFTlcA407"
      },
      "source": [
        "e.g., the following piece of code is to extract 500 PDF files (with index 0-499)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UGuhBjCZPeA"
      },
      "source": [
        "new_list1={}\n",
        "\n",
        "for file in file_list[:500]:   # start from 0, up to 500 but not including\n",
        "  print(file)\n",
        "    \n",
        "  if file.endswith(\".pdf\"):\n",
        "    try:\n",
        "      with pdfplumber.open(file) as pdf:\n",
        "        fulltext = \"\"\n",
        "        try:\n",
        "          for page in pdf.pages:\n",
        "            try:\n",
        "              text = page.extract_text()\n",
        "              fulltext += str(text)\n",
        "            except:\n",
        "              fulltext = \"\"\n",
        "              print(\"Blank\")\n",
        "            finally:\n",
        "              pass\n",
        "        except:\n",
        "          pass\n",
        "    except:\n",
        "      pass\n",
        "        \n",
        "        \n",
        "    new_list1[file]=(fulltext) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i90NhSkBBaZP"
      },
      "source": [
        "check the number of files in the first list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIxYwCQF1yLR"
      },
      "source": [
        "len(new_list1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8mYbEmiKNAR"
      },
      "source": [
        "### Step 3.2: List 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAAT2LUCtr7U"
      },
      "source": [
        "new_list2={}\n",
        "\n",
        "for file in file_list[500:1000]:   # start from 500, up to 1000 but not including\n",
        "  print(file)\n",
        "    \n",
        "  if file.endswith(\".pdf\"):\n",
        "    try:\n",
        "      with pdfplumber.open(file) as pdf:\n",
        "        fulltext = \"\"\n",
        "        try:\n",
        "          for page in pdf.pages:\n",
        "            try:\n",
        "              text = page.extract_text()\n",
        "              fulltext += str(text)\n",
        "            except:\n",
        "              fulltext = \"\"\n",
        "              print(\"Blank\")\n",
        "            finally:\n",
        "              pass\n",
        "        except:\n",
        "          pass\n",
        "    except:\n",
        "      pass\n",
        "        \n",
        "        \n",
        "    new_list2[file]=(fulltext) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9sIIRD6B_eL"
      },
      "source": [
        "len(new_list2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pmKCKXvKRzb"
      },
      "source": [
        "### Step 3.3: List 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrUmBC9Wt1Yk"
      },
      "source": [
        "new_list3={}\n",
        "\n",
        "for file in file_list[1000:]:   # start from 1000, to the end\n",
        "  print(file)\n",
        "    \n",
        "  if file.endswith(\".pdf\"):\n",
        "    try:\n",
        "      with pdfplumber.open(file) as pdf:\n",
        "        fulltext = \"\"\n",
        "        try:\n",
        "          for page in pdf.pages:\n",
        "            try:\n",
        "              text = page.extract_text()\n",
        "              fulltext += str(text)\n",
        "            except:\n",
        "              fulltext = \"\"\n",
        "              print(\"Blank\")\n",
        "            finally:\n",
        "              pass\n",
        "        except:\n",
        "          pass\n",
        "    except:\n",
        "      pass\n",
        "        \n",
        "        \n",
        "    new_list3[file]=(fulltext) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CkPkLl6B9G6"
      },
      "source": [
        "len(new_list3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNsmRIQCCmGM"
      },
      "source": [
        "So far, all PDF files in the selected folder are extracted. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z138QCYECuJE"
      },
      "source": [
        "## Step 4: Save each generated list as a json file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM31ikmIckTa"
      },
      "source": [
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YOn1qLbC_a0"
      },
      "source": [
        "### Step 4.1: List 1\n",
        "\n",
        "##### Step 4.1.1: Create json files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPJKUQ8Ea9U6"
      },
      "source": [
        "json1 = json.dumps(new_list1)    # convert a Python object into a json string\n",
        "\n",
        "# open file for writing, \"w\" \n",
        "f = open(\"2018_1.json\",\"w\")      # create a new json file, called 2021_1\n",
        "\n",
        "# write json object to file\n",
        "f.write(json1)                   \n",
        "\n",
        "# close file\n",
        "f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y1Zt_GaExyW"
      },
      "source": [
        "##### Step 4.1.2: Save & upload the created json files\n",
        "\n",
        "##### (1) right click to save the json file into my local machine\n",
        "\n",
        "##### (2) upload the json file to my Google Drive\n",
        "\n",
        "##### (3) open the json file in Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhI4F7p_a-AT"
      },
      "source": [
        "#Open JSON file\n",
        "a1 = open('/content/drive/MyDrive/NEW dataset - MAJ/2018/2018_1.json', 'r')\n",
        "output1 = a1.read()\n",
        "\n",
        "#Convert JSON file to python dictionary\n",
        "final_list1 = json.loads(output1)\n",
        "final_list1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu_4tFtuhx1u"
      },
      "source": [
        "type(final_list1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhQesR6VMD9F"
      },
      "source": [
        "### Step 4.2: List 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj4xme7LtTVa"
      },
      "source": [
        "json2 = json.dumps(new_list2)\n",
        "\n",
        "# open file for writing, \"w\" \n",
        "f = open(\"2018_2.json\",\"w\")\n",
        "\n",
        "# write json object to file\n",
        "f.write(json2)\n",
        "\n",
        "# close file\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEpLsAjvtUg0"
      },
      "source": [
        "#Open JSON file\n",
        "a2 = open('/content/drive/MyDrive/NEW dataset - MAJ/2018/2018_2.json', 'r')\n",
        "output2 = a2.read()\n",
        "\n",
        "#Convert JSON file to python dictionary\n",
        "final_list2 = json.loads(output2)\n",
        "final_list2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guOc46mLNEcl"
      },
      "source": [
        "### Step 4.3: List 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjD349vjuLKG"
      },
      "source": [
        "json3 = json.dumps(new_list3)\n",
        "\n",
        "# open file for writing, \"w\" \n",
        "f = open(\"2018_3.json\",\"w\")\n",
        "\n",
        "# write json object to file\n",
        "f.write(json3)\n",
        "\n",
        "# close file\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyZv4RUKuOXI"
      },
      "source": [
        "#Open JSON file\n",
        "a3 = open('/content/drive/MyDrive/NEW dataset - MAJ/2018/2018_3.json', 'r')\n",
        "output3 = a3.read()\n",
        "\n",
        "#Convert JSON file to python dictionary\n",
        "final_list3 = json.loads(output3)\n",
        "final_list3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIyIQ5-eOBkN"
      },
      "source": [
        "So far, all three json files are saved in Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmIQsoA1FzSN"
      },
      "source": [
        "## Step 5: Combine all three individual dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoRAv9s6uhUC"
      },
      "source": [
        "#Combine all dictionaries\n",
        "final_list = {**final_list1, **final_list2, **final_list3}   # number of dictionaries to be combined can be changed here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il2oO_VbumyA"
      },
      "source": [
        "json_final = json.dumps(final_list)\n",
        "\n",
        "# open file for writing, \"w\" \n",
        "f = open(\"2018_final.json\",\"w\")\n",
        "\n",
        "# write json object to file\n",
        "f.write(json_final)\n",
        "\n",
        "# close file\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDGMyZEEusvO"
      },
      "source": [
        "#Open JSON file\n",
        "a = open('/content/drive/MyDrive/NEW dataset - MAJ/2018/2018_final.json', 'r')\n",
        "output = a.read()\n",
        "\n",
        "#Convert JSON file to python dictionary\n",
        "final_list = json.loads(output)\n",
        "final_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYF6h2hadqds"
      },
      "source": [
        "len(final_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gH6_qLtHkzw"
      },
      "source": [
        "# All done!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvnCMOQGCWvS"
      },
      "source": [
        "pagen=[]\n",
        "for file in file_list:\n",
        "  # print(file)\n",
        "  n=(file.split('.pdf',0))\n",
        "  \n",
        "  \n",
        "  if file.endswith(\".pdf\"):\n",
        "    try:\n",
        "      with pdfplumber.open(file) as pdf:\n",
        "        pagen.append(len(pdf.pages))\n",
        "    except:\n",
        "        pagen.append('N/A')\n",
        "    finally:  \n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bJUTa3rCh6r"
      },
      "source": [
        "#Readability + word count\n",
        "sc = []\n",
        "gf=[]\n",
        "kg=[]\n",
        "ke=[]\n",
        "dc=[]\n",
        "word_count=[]\n",
        "\n",
        "for k,v in final_list.items():\n",
        "  # print(k)\n",
        "  # print(len(v))\n",
        "  try:\n",
        "    sc.append(textstat.sentence_count(v))\n",
        "    gf.append(textstat.gunning_fog(v))\n",
        "    kg.append(textstat.flesch_kincaid_grade(v))\n",
        "    ke.append(textstat.flesch_reading_ease(v))\n",
        "    dc.append(textstat.dale_chall_readability_score(v))\n",
        "    word_count.append(textstat.lexicon_count(v, removepunct=True))\n",
        "    \n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48zunr9JCmEe"
      },
      "source": [
        "asxcode=[]\n",
        "fyear=[]\n",
        "\n",
        "\n",
        "for file in file_list:\n",
        "  try:\n",
        "    n=(file.split('.pdf',0))\n",
        "    asxcode.append((file.rsplit('/content/drive/MyDrive/ASX annual reports/2004 (1359 firms)/')[1][0:3]).upper())\n",
        "    fyear.append((file.rsplit('/content/drive/MyDrive/ASX annual reports/2004 (1359 firms)/')[1][3:5]))\n",
        "    \n",
        "  except:\n",
        "    pass\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gsj7y4hCmkT"
      },
      "source": [
        "from itertools import zip_longest\n",
        "data = [asxcode, fyear, sc, gf, kg, ke, dc, word_count, pagen]\n",
        "export_data = zip_longest(*data, fillvalue = '')\n",
        "with open('/content/drive/MyDrive/MAJ trial/NEW 2004 results.csv', 'w', encoding=\"ISO-8859-1\", newline='') as file:\n",
        "      write = csv.writer(file)\n",
        "      write.writerow(('asxcode', 'fyear', 'sc', 'gf', 'kg', 'ke', 'dc', 'word_count', 'pagen'))\n",
        "      write.writerows(export_data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}